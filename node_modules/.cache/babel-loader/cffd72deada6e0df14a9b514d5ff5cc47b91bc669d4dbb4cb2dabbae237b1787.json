{"ast":null,"code":"/*\n * Copyright (c) AXA Group Operations Spain S.A.\n *\n * Permission is hereby granted, free of charge, to any person obtaining\n * a copy of this software and associated documentation files (the\n * \"Software\"), to deal in the Software without restriction, including\n * without limitation the rights to use, copy, modify, merge, publish,\n * distribute, sublicense, and/or sell copies of the Software, and to\n * permit persons to whom the Software is furnished to do so, subject to\n * the following conditions:\n *\n * The above copyright notice and this permission notice shall be\n * included in all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n */\n\nconst {\n  BaseStemmer\n} = require('@nlpjs/core');\nconst {\n  tokenize,\n  stemWord\n} = require('./korean-tokenizer');\nconst {\n  initDicts,\n  dictionary\n} = require('./korean-dictionary');\nconst TokenizerKo = require('./tokenizer-ko');\nconst NormalizerKo = require('./normalizer-ko');\nconst preendings = ['하고있는', '합니까', '습니다', '자에게', '머에게', '밍하는', '에게', '자에', '자의', '하고', '하는', '자는', '자가', '습니', '읍시', '는다', '으냐', 'ᆻ다', '하는', 'ᆻ', '처', '다', '요', '까', '니', '시', '를', '는', '에', '을', '이', '의', '를', '에', '는', '밍'];\nclass StemmerKo extends BaseStemmer {\n  constructor(container) {\n    super(container);\n    this.name = 'stemmer-ko';\n    this.tokenizer = new TokenizerKo();\n    this.normalizer = new NormalizerKo();\n  }\n  isHangulChar(ch) {\n    const regex = /[\\u1100-\\u11FF\\u302E\\u302F\\u3131-\\u318E\\u3200-\\u321E\\u3260-\\u327E\\uA960-\\uA97C\\uAC00-\\uD7A3\\uD7B0-\\uD7C6\\uD7CB-\\uD7FB\\uFFA0-\\uFFBE\\uFFC2-\\uFFC7\\uFFCA-\\uFFCF\\uFFD2-\\uFFD7\\uFFDA-\\uFFDC]/g;\n    return regex.test(ch);\n  }\n  normalize(text) {\n    return text\n    // .normalize('NFD')\n    .replace(/[\\u0300-\\u036f]/g, '').replace(/까?/g, '').toLowerCase();\n  }\n  tokenize(text) {\n    const tokens = text.split(/[\\s,.!?;:([\\]'\"¡¿)/]+/).filter(x => x);\n    const result = [];\n    for (let i = 0; i < tokens.length; i += 1) {\n      const token = tokens[i];\n      let word = token[0];\n      let isHangul = this.isHangulChar(token[0]);\n      for (let j = 1; j < token.length; j += 1) {\n        const char = token[j];\n        const newIsHangul = this.isHangulChar(char);\n        if (newIsHangul !== isHangul) {\n          result.push(word);\n          word = char;\n          isHangul = newIsHangul;\n        } else {\n          word += char;\n        }\n      }\n      result.push(word);\n    }\n    return result;\n  }\n  prestem(word) {\n    for (let i = 0; i < preendings.length; i += 1) {\n      if (word.endsWith(preendings[i])) {\n        return word.slice(0, -preendings[i].length);\n      }\n    }\n    if (word.endsWith('습니다')) {\n      return word.slice(0, -2);\n    }\n    if (word.endsWith('세요')) {\n      return word.slice(0, -2);\n    }\n    if (word.endsWith('으러')) {\n      return word.slice(0, -2);\n    }\n    if (word.endsWith('러')) {\n      return word.slice(0, -1);\n    }\n    if (word.endsWith('고')) {\n      return word.slice(0, -1);\n    }\n    if (word.endsWith('네')) {\n      return word.slice(0, -1);\n    }\n    if (word.endsWith('다')) {\n      return word.slice(0, -1);\n    }\n    if (word.endsWith('자')) {\n      return word.slice(0, -1);\n    }\n    return word;\n  }\n  innerStem() {\n    initDicts();\n    const word = this.getCurrent();\n    const token = stemWord(this.prestem(word.trim()));\n    const value = dictionary[token];\n    this.setCurrent(value && value.root ? value.root : token);\n  }\n  async stem(text, input) {\n    initDicts();\n    const inputText = typeof text === 'string' ? text : input.utterance || input.text;\n    const newText = this.tokenizer.tokenize(this.normalizer.normalize(inputText)).join(' ');\n    const tokens = tokenize(this.normalizer.normalize(newText)).map(x => stemWord(this.prestem(x.trim()))).filter(x => x);\n    const result = [];\n    for (let i = 0; i < tokens.length; i += 1) {\n      const value = dictionary[tokens[i]];\n      if (value && value.root) {\n        result.push(value.root);\n      } else {\n        result.push(tokens[i]);\n      }\n    }\n    return result;\n  }\n  async run(srcInput) {\n    const input = srcInput;\n    const locale = input.locale || 'en';\n    const stemmer = this.container.get(`stemmer-${locale}`) || this;\n    input.tokens = await stemmer.stem(input.text || input.tokens.join(' '), input);\n    return input;\n  }\n  tokenizeAndStem(text) {\n    initDicts();\n    const newText = this.tokenize(this.normalize(text)).join(' ');\n    const tokens = tokenize(this.normalize(newText)).map(x => stemWord(this.prestem(x.trim()))).filter(x => x);\n    const result = [];\n    for (let i = 0; i < tokens.length; i += 1) {\n      const value = dictionary[tokens[i]];\n      if (value && value.root) {\n        result.push(value.root);\n      } else {\n        result.push(tokens[i]);\n      }\n    }\n    return result;\n  }\n}\nmodule.exports = StemmerKo;","map":{"version":3,"names":["BaseStemmer","require","tokenize","stemWord","initDicts","dictionary","TokenizerKo","NormalizerKo","preendings","StemmerKo","constructor","container","name","tokenizer","normalizer","isHangulChar","ch","regex","test","normalize","text","replace","toLowerCase","tokens","split","filter","x","result","i","length","token","word","isHangul","j","char","newIsHangul","push","prestem","endsWith","slice","innerStem","getCurrent","trim","value","setCurrent","root","stem","input","inputText","utterance","newText","join","map","run","srcInput","locale","stemmer","get","tokenizeAndStem","module","exports"],"sources":["/Users/zyq/Desktop/大二下/暑期实习/moonshot project/node_modules/@nlpjs/lang-ko/src/stemmer-ko.js"],"sourcesContent":["/*\n * Copyright (c) AXA Group Operations Spain S.A.\n *\n * Permission is hereby granted, free of charge, to any person obtaining\n * a copy of this software and associated documentation files (the\n * \"Software\"), to deal in the Software without restriction, including\n * without limitation the rights to use, copy, modify, merge, publish,\n * distribute, sublicense, and/or sell copies of the Software, and to\n * permit persons to whom the Software is furnished to do so, subject to\n * the following conditions:\n *\n * The above copyright notice and this permission notice shall be\n * included in all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n */\n\nconst { BaseStemmer } = require('@nlpjs/core');\nconst { tokenize, stemWord } = require('./korean-tokenizer');\nconst { initDicts, dictionary } = require('./korean-dictionary');\nconst TokenizerKo = require('./tokenizer-ko');\nconst NormalizerKo = require('./normalizer-ko');\n\nconst preendings = [\n  '하고있는',\n  '합니까',\n  '습니다',\n  '자에게',\n  '머에게',\n  '밍하는',\n  '에게',\n  '자에',\n  '자의',\n  '하고',\n  '하는',\n  '자는',\n  '자가',\n  '습니',\n  '읍시',\n  '는다',\n  '으냐',\n  'ᆻ다',\n  '하는',\n  'ᆻ',\n  '처',\n  '다',\n  '요',\n  '까',\n  '니',\n  '시',\n  '를',\n  '는',\n  '에',\n  '을',\n  '이',\n  '의',\n  '를',\n  '에',\n  '는',\n  '밍',\n];\n\nclass StemmerKo extends BaseStemmer {\n  constructor(container) {\n    super(container);\n    this.name = 'stemmer-ko';\n    this.tokenizer = new TokenizerKo();\n    this.normalizer = new NormalizerKo();\n  }\n\n  isHangulChar(ch) {\n    const regex =\n      /[\\u1100-\\u11FF\\u302E\\u302F\\u3131-\\u318E\\u3200-\\u321E\\u3260-\\u327E\\uA960-\\uA97C\\uAC00-\\uD7A3\\uD7B0-\\uD7C6\\uD7CB-\\uD7FB\\uFFA0-\\uFFBE\\uFFC2-\\uFFC7\\uFFCA-\\uFFCF\\uFFD2-\\uFFD7\\uFFDA-\\uFFDC]/g;\n    return regex.test(ch);\n  }\n\n  normalize(text) {\n    return (\n      text\n        // .normalize('NFD')\n        .replace(/[\\u0300-\\u036f]/g, '')\n        .replace(/까?/g, '')\n        .toLowerCase()\n    );\n  }\n\n  tokenize(text) {\n    const tokens = text.split(/[\\s,.!?;:([\\]'\"¡¿)/]+/).filter((x) => x);\n    const result = [];\n    for (let i = 0; i < tokens.length; i += 1) {\n      const token = tokens[i];\n      let word = token[0];\n      let isHangul = this.isHangulChar(token[0]);\n      for (let j = 1; j < token.length; j += 1) {\n        const char = token[j];\n        const newIsHangul = this.isHangulChar(char);\n        if (newIsHangul !== isHangul) {\n          result.push(word);\n          word = char;\n          isHangul = newIsHangul;\n        } else {\n          word += char;\n        }\n      }\n      result.push(word);\n    }\n    return result;\n  }\n\n  prestem(word) {\n    for (let i = 0; i < preendings.length; i += 1) {\n      if (word.endsWith(preendings[i])) {\n        return word.slice(0, -preendings[i].length);\n      }\n    }\n    if (word.endsWith('습니다')) {\n      return word.slice(0, -2);\n    }\n    if (word.endsWith('세요')) {\n      return word.slice(0, -2);\n    }\n    if (word.endsWith('으러')) {\n      return word.slice(0, -2);\n    }\n    if (word.endsWith('러')) {\n      return word.slice(0, -1);\n    }\n    if (word.endsWith('고')) {\n      return word.slice(0, -1);\n    }\n    if (word.endsWith('네')) {\n      return word.slice(0, -1);\n    }\n    if (word.endsWith('다')) {\n      return word.slice(0, -1);\n    }\n    if (word.endsWith('자')) {\n      return word.slice(0, -1);\n    }\n    return word;\n  }\n\n  innerStem() {\n    initDicts();\n    const word = this.getCurrent();\n    const token = stemWord(this.prestem(word.trim()));\n    const value = dictionary[token];\n    this.setCurrent(value && value.root ? value.root : token);\n  }\n\n  async stem(text, input) {\n    initDicts();\n    const inputText =\n      typeof text === 'string' ? text : input.utterance || input.text;\n    const newText = this.tokenizer\n      .tokenize(this.normalizer.normalize(inputText))\n      .join(' ');\n    const tokens = tokenize(this.normalizer.normalize(newText))\n      .map((x) => stemWord(this.prestem(x.trim())))\n      .filter((x) => x);\n    const result = [];\n    for (let i = 0; i < tokens.length; i += 1) {\n      const value = dictionary[tokens[i]];\n      if (value && value.root) {\n        result.push(value.root);\n      } else {\n        result.push(tokens[i]);\n      }\n    }\n    return result;\n  }\n\n  async run(srcInput) {\n    const input = srcInput;\n    const locale = input.locale || 'en';\n    const stemmer = this.container.get(`stemmer-${locale}`) || this;\n    input.tokens = await stemmer.stem(\n      input.text || input.tokens.join(' '),\n      input\n    );\n    return input;\n  }\n\n  tokenizeAndStem(text) {\n    initDicts();\n    const newText = this.tokenize(this.normalize(text)).join(' ');\n    const tokens = tokenize(this.normalize(newText))\n      .map((x) => stemWord(this.prestem(x.trim())))\n      .filter((x) => x);\n    const result = [];\n    for (let i = 0; i < tokens.length; i += 1) {\n      const value = dictionary[tokens[i]];\n      if (value && value.root) {\n        result.push(value.root);\n      } else {\n        result.push(tokens[i]);\n      }\n    }\n    return result;\n  }\n}\n\nmodule.exports = StemmerKo;\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,MAAM;EAAEA;AAAY,CAAC,GAAGC,OAAO,CAAC,aAAa,CAAC;AAC9C,MAAM;EAAEC,QAAQ;EAAEC;AAAS,CAAC,GAAGF,OAAO,CAAC,oBAAoB,CAAC;AAC5D,MAAM;EAAEG,SAAS;EAAEC;AAAW,CAAC,GAAGJ,OAAO,CAAC,qBAAqB,CAAC;AAChE,MAAMK,WAAW,GAAGL,OAAO,CAAC,gBAAgB,CAAC;AAC7C,MAAMM,YAAY,GAAGN,OAAO,CAAC,iBAAiB,CAAC;AAE/C,MAAMO,UAAU,GAAG,CACjB,MAAM,EACN,KAAK,EACL,KAAK,EACL,KAAK,EACL,KAAK,EACL,KAAK,EACL,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,IAAI,EACJ,GAAG,EACH,GAAG,EACH,GAAG,EACH,GAAG,EACH,GAAG,EACH,GAAG,EACH,GAAG,EACH,KAAK,EACL,KAAK,EACL,IAAI,EACJ,KAAK,EACL,IAAI,EACJ,GAAG,EACH,GAAG,EACH,GAAG,EACH,GAAG,EACH,GAAG,CACJ;AAED,MAAMC,SAAS,SAAST,WAAW,CAAC;EAClCU,WAAWA,CAACC,SAAS,EAAE;IACrB,KAAK,CAACA,SAAS,CAAC;IAChB,IAAI,CAACC,IAAI,GAAG,YAAY;IACxB,IAAI,CAACC,SAAS,GAAG,IAAIP,WAAW,CAAC,CAAC;IAClC,IAAI,CAACQ,UAAU,GAAG,IAAIP,YAAY,CAAC,CAAC;EACtC;EAEAQ,YAAYA,CAACC,EAAE,EAAE;IACf,MAAMC,KAAK,GACT,0LAA0L;IAC5L,OAAOA,KAAK,CAACC,IAAI,CAACF,EAAE,CAAC;EACvB;EAEAG,SAASA,CAACC,IAAI,EAAE;IACd,OACEA;IACE;IAAA,CACCC,OAAO,CAAC,kBAAkB,EAAE,EAAE,CAAC,CAC/BA,OAAO,CAAC,KAAK,EAAE,EAAE,CAAC,CAClBC,WAAW,CAAC,CAAC;EAEpB;EAEApB,QAAQA,CAACkB,IAAI,EAAE;IACb,MAAMG,MAAM,GAAGH,IAAI,CAACI,KAAK,CAAC,uBAAuB,CAAC,CAACC,MAAM,CAAEC,CAAC,IAAKA,CAAC,CAAC;IACnE,MAAMC,MAAM,GAAG,EAAE;IACjB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGL,MAAM,CAACM,MAAM,EAAED,CAAC,IAAI,CAAC,EAAE;MACzC,MAAME,KAAK,GAAGP,MAAM,CAACK,CAAC,CAAC;MACvB,IAAIG,IAAI,GAAGD,KAAK,CAAC,CAAC,CAAC;MACnB,IAAIE,QAAQ,GAAG,IAAI,CAACjB,YAAY,CAACe,KAAK,CAAC,CAAC,CAAC,CAAC;MAC1C,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,KAAK,CAACD,MAAM,EAAEI,CAAC,IAAI,CAAC,EAAE;QACxC,MAAMC,IAAI,GAAGJ,KAAK,CAACG,CAAC,CAAC;QACrB,MAAME,WAAW,GAAG,IAAI,CAACpB,YAAY,CAACmB,IAAI,CAAC;QAC3C,IAAIC,WAAW,KAAKH,QAAQ,EAAE;UAC5BL,MAAM,CAACS,IAAI,CAACL,IAAI,CAAC;UACjBA,IAAI,GAAGG,IAAI;UACXF,QAAQ,GAAGG,WAAW;QACxB,CAAC,MAAM;UACLJ,IAAI,IAAIG,IAAI;QACd;MACF;MACAP,MAAM,CAACS,IAAI,CAACL,IAAI,CAAC;IACnB;IACA,OAAOJ,MAAM;EACf;EAEAU,OAAOA,CAACN,IAAI,EAAE;IACZ,KAAK,IAAIH,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGpB,UAAU,CAACqB,MAAM,EAAED,CAAC,IAAI,CAAC,EAAE;MAC7C,IAAIG,IAAI,CAACO,QAAQ,CAAC9B,UAAU,CAACoB,CAAC,CAAC,CAAC,EAAE;QAChC,OAAOG,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC/B,UAAU,CAACoB,CAAC,CAAC,CAACC,MAAM,CAAC;MAC7C;IACF;IACA,IAAIE,IAAI,CAACO,QAAQ,CAAC,KAAK,CAAC,EAAE;MACxB,OAAOP,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC1B;IACA,IAAIR,IAAI,CAACO,QAAQ,CAAC,IAAI,CAAC,EAAE;MACvB,OAAOP,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC1B;IACA,IAAIR,IAAI,CAACO,QAAQ,CAAC,IAAI,CAAC,EAAE;MACvB,OAAOP,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC1B;IACA,IAAIR,IAAI,CAACO,QAAQ,CAAC,GAAG,CAAC,EAAE;MACtB,OAAOP,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC1B;IACA,IAAIR,IAAI,CAACO,QAAQ,CAAC,GAAG,CAAC,EAAE;MACtB,OAAOP,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC1B;IACA,IAAIR,IAAI,CAACO,QAAQ,CAAC,GAAG,CAAC,EAAE;MACtB,OAAOP,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC1B;IACA,IAAIR,IAAI,CAACO,QAAQ,CAAC,GAAG,CAAC,EAAE;MACtB,OAAOP,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC1B;IACA,IAAIR,IAAI,CAACO,QAAQ,CAAC,GAAG,CAAC,EAAE;MACtB,OAAOP,IAAI,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC1B;IACA,OAAOR,IAAI;EACb;EAEAS,SAASA,CAAA,EAAG;IACVpC,SAAS,CAAC,CAAC;IACX,MAAM2B,IAAI,GAAG,IAAI,CAACU,UAAU,CAAC,CAAC;IAC9B,MAAMX,KAAK,GAAG3B,QAAQ,CAAC,IAAI,CAACkC,OAAO,CAACN,IAAI,CAACW,IAAI,CAAC,CAAC,CAAC,CAAC;IACjD,MAAMC,KAAK,GAAGtC,UAAU,CAACyB,KAAK,CAAC;IAC/B,IAAI,CAACc,UAAU,CAACD,KAAK,IAAIA,KAAK,CAACE,IAAI,GAAGF,KAAK,CAACE,IAAI,GAAGf,KAAK,CAAC;EAC3D;EAEA,MAAMgB,IAAIA,CAAC1B,IAAI,EAAE2B,KAAK,EAAE;IACtB3C,SAAS,CAAC,CAAC;IACX,MAAM4C,SAAS,GACb,OAAO5B,IAAI,KAAK,QAAQ,GAAGA,IAAI,GAAG2B,KAAK,CAACE,SAAS,IAAIF,KAAK,CAAC3B,IAAI;IACjE,MAAM8B,OAAO,GAAG,IAAI,CAACrC,SAAS,CAC3BX,QAAQ,CAAC,IAAI,CAACY,UAAU,CAACK,SAAS,CAAC6B,SAAS,CAAC,CAAC,CAC9CG,IAAI,CAAC,GAAG,CAAC;IACZ,MAAM5B,MAAM,GAAGrB,QAAQ,CAAC,IAAI,CAACY,UAAU,CAACK,SAAS,CAAC+B,OAAO,CAAC,CAAC,CACxDE,GAAG,CAAE1B,CAAC,IAAKvB,QAAQ,CAAC,IAAI,CAACkC,OAAO,CAACX,CAAC,CAACgB,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAC5CjB,MAAM,CAAEC,CAAC,IAAKA,CAAC,CAAC;IACnB,MAAMC,MAAM,GAAG,EAAE;IACjB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGL,MAAM,CAACM,MAAM,EAAED,CAAC,IAAI,CAAC,EAAE;MACzC,MAAMe,KAAK,GAAGtC,UAAU,CAACkB,MAAM,CAACK,CAAC,CAAC,CAAC;MACnC,IAAIe,KAAK,IAAIA,KAAK,CAACE,IAAI,EAAE;QACvBlB,MAAM,CAACS,IAAI,CAACO,KAAK,CAACE,IAAI,CAAC;MACzB,CAAC,MAAM;QACLlB,MAAM,CAACS,IAAI,CAACb,MAAM,CAACK,CAAC,CAAC,CAAC;MACxB;IACF;IACA,OAAOD,MAAM;EACf;EAEA,MAAM0B,GAAGA,CAACC,QAAQ,EAAE;IAClB,MAAMP,KAAK,GAAGO,QAAQ;IACtB,MAAMC,MAAM,GAAGR,KAAK,CAACQ,MAAM,IAAI,IAAI;IACnC,MAAMC,OAAO,GAAG,IAAI,CAAC7C,SAAS,CAAC8C,GAAG,CAAC,WAAWF,MAAM,EAAE,CAAC,IAAI,IAAI;IAC/DR,KAAK,CAACxB,MAAM,GAAG,MAAMiC,OAAO,CAACV,IAAI,CAC/BC,KAAK,CAAC3B,IAAI,IAAI2B,KAAK,CAACxB,MAAM,CAAC4B,IAAI,CAAC,GAAG,CAAC,EACpCJ,KACF,CAAC;IACD,OAAOA,KAAK;EACd;EAEAW,eAAeA,CAACtC,IAAI,EAAE;IACpBhB,SAAS,CAAC,CAAC;IACX,MAAM8C,OAAO,GAAG,IAAI,CAAChD,QAAQ,CAAC,IAAI,CAACiB,SAAS,CAACC,IAAI,CAAC,CAAC,CAAC+B,IAAI,CAAC,GAAG,CAAC;IAC7D,MAAM5B,MAAM,GAAGrB,QAAQ,CAAC,IAAI,CAACiB,SAAS,CAAC+B,OAAO,CAAC,CAAC,CAC7CE,GAAG,CAAE1B,CAAC,IAAKvB,QAAQ,CAAC,IAAI,CAACkC,OAAO,CAACX,CAAC,CAACgB,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAC5CjB,MAAM,CAAEC,CAAC,IAAKA,CAAC,CAAC;IACnB,MAAMC,MAAM,GAAG,EAAE;IACjB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGL,MAAM,CAACM,MAAM,EAAED,CAAC,IAAI,CAAC,EAAE;MACzC,MAAMe,KAAK,GAAGtC,UAAU,CAACkB,MAAM,CAACK,CAAC,CAAC,CAAC;MACnC,IAAIe,KAAK,IAAIA,KAAK,CAACE,IAAI,EAAE;QACvBlB,MAAM,CAACS,IAAI,CAACO,KAAK,CAACE,IAAI,CAAC;MACzB,CAAC,MAAM;QACLlB,MAAM,CAACS,IAAI,CAACb,MAAM,CAACK,CAAC,CAAC,CAAC;MACxB;IACF;IACA,OAAOD,MAAM;EACf;AACF;AAEAgC,MAAM,CAACC,OAAO,GAAGnD,SAAS","ignoreList":[]},"metadata":{},"sourceType":"script","externalDependencies":[]}